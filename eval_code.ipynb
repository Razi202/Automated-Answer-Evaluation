{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\Intag\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748365947f0c4b8fbc80fbd23346b308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sq = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What sits on top of the Main Building at Notre Dame?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq['train'][4]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a golden statue of the Virgin Mary'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq['train'][4]['answers']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./eval_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['multilabel_id', 'query', 'filters', 'gold_answers', 'answer',\n",
       "       'context', 'exact_match', 'f1', 'rank', 'document_id',\n",
       "       'gold_document_ids', 'offsets_in_document', 'gold_offsets_in_documents',\n",
       "       'type', 'node', 'eval_mode', 'index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['f1', 'rank', 'document_id',\n",
    "       'gold_document_ids', 'offsets_in_document', 'gold_offsets_in_documents',\n",
    "       'type', 'node', 'eval_mode', 'index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('not_given')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-e8d162cfe9c9>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer'][i] = re.sub(r'[^\\w\\s]', '', df['answer'][i])\n",
      "<ipython-input-13-e8d162cfe9c9>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer'][i] = df['answer'][i].lower()\n"
     ]
    }
   ],
   "source": [
    "correct = []\n",
    "for i in range(len(df['gold_answers'])):\n",
    "    try:\n",
    "        temp = ast.literal_eval(df['gold_answers'][i])\n",
    "    except:\n",
    "        pass\n",
    "    df['answer'][i] = re.sub(r'[^\\w\\s]', '', df['answer'][i])\n",
    "    df['answer'][i] = df['answer'][i].lower()\n",
    "    for j in range(len(temp)):\n",
    "        temp[j] = re.sub(r'[^\\w\\s]', '', temp[j])\n",
    "        temp[j] = temp[j].lower()\n",
    "    correct.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gold_answers'] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multilabel_id</th>\n",
       "      <th>query</th>\n",
       "      <th>filters</th>\n",
       "      <th>gold_answers</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5616814845033682107</td>\n",
       "      <td>who is written in the book of life</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[all people considered righteous before god, e...</td>\n",
       "      <td>not_given</td>\n",
       "      <td>not_given</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5616814845033682107</td>\n",
       "      <td>who is written in the book of life</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[all people considered righteous before god, e...</td>\n",
       "      <td>those whose names are written in the book of l...</td>\n",
       "      <td>ohn of Patmos. As described, only those whose ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5616814845033682107</td>\n",
       "      <td>who is written in the book of life</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[all people considered righteous before god, e...</td>\n",
       "      <td>only the names of the righteous</td>\n",
       "      <td>. The Psalmist likewise speaks of the Book of ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5616814845033682107</td>\n",
       "      <td>who is written in the book of life</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[all people considered righteous before god, e...</td>\n",
       "      <td>those who are found written in the book and wh...</td>\n",
       "      <td>those who are found written in the book and wh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4885963241536934817</td>\n",
       "      <td>who was the girl in the video brenda got a baby</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[ethel edy proctor]</td>\n",
       "      <td>her cousin</td>\n",
       "      <td>ng a story in the newspaper of a 12-year-old g...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-2582649975336248842</td>\n",
       "      <td>the fertile crescent is located between what t...</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[tigris and euphrates rivers]</td>\n",
       "      <td>not_given</td>\n",
       "      <td>not_given</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2968376238709049184</td>\n",
       "      <td>how did the fertile crescent get its nickname</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[popularized by university of chicago archaeol...</td>\n",
       "      <td>not_given</td>\n",
       "      <td>not_given</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2968376238709049184</td>\n",
       "      <td>how did the fertile crescent get its nickname</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[popularized by university of chicago archaeol...</td>\n",
       "      <td>the cradle of civilization</td>\n",
       "      <td>atelevel societies. This has earned the region...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2968376238709049184</td>\n",
       "      <td>how did the fertile crescent get its nickname</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[popularized by university of chicago archaeol...</td>\n",
       "      <td>the cradle of civilization</td>\n",
       "      <td>e Fertile Crescent in red The Fertile Crescent...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2968376238709049184</td>\n",
       "      <td>how did the fertile crescent get its nickname</td>\n",
       "      <td>b'null'</td>\n",
       "      <td>[popularized by university of chicago archaeol...</td>\n",
       "      <td>the fertile crescent has an impressive record ...</td>\n",
       "      <td>Cuneiform law History of the Middle East The F...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          multilabel_id                                              query  \\\n",
       "0   5616814845033682107                 who is written in the book of life   \n",
       "1   5616814845033682107                 who is written in the book of life   \n",
       "2   5616814845033682107                 who is written in the book of life   \n",
       "3   5616814845033682107                 who is written in the book of life   \n",
       "4  -4885963241536934817    who was the girl in the video brenda got a baby   \n",
       "..                  ...                                                ...   \n",
       "95 -2582649975336248842  the fertile crescent is located between what t...   \n",
       "96  2968376238709049184      how did the fertile crescent get its nickname   \n",
       "97  2968376238709049184      how did the fertile crescent get its nickname   \n",
       "98  2968376238709049184      how did the fertile crescent get its nickname   \n",
       "99  2968376238709049184      how did the fertile crescent get its nickname   \n",
       "\n",
       "    filters                                       gold_answers  \\\n",
       "0   b'null'  [all people considered righteous before god, e...   \n",
       "1   b'null'  [all people considered righteous before god, e...   \n",
       "2   b'null'  [all people considered righteous before god, e...   \n",
       "3   b'null'  [all people considered righteous before god, e...   \n",
       "4   b'null'                                [ethel edy proctor]   \n",
       "..      ...                                                ...   \n",
       "95  b'null'                      [tigris and euphrates rivers]   \n",
       "96  b'null'  [popularized by university of chicago archaeol...   \n",
       "97  b'null'  [popularized by university of chicago archaeol...   \n",
       "98  b'null'  [popularized by university of chicago archaeol...   \n",
       "99  b'null'  [popularized by university of chicago archaeol...   \n",
       "\n",
       "                                               answer  \\\n",
       "0                                           not_given   \n",
       "1   those whose names are written in the book of l...   \n",
       "2                     only the names of the righteous   \n",
       "3   those who are found written in the book and wh...   \n",
       "4                                          her cousin   \n",
       "..                                                ...   \n",
       "95                                          not_given   \n",
       "96                                          not_given   \n",
       "97                         the cradle of civilization   \n",
       "98                         the cradle of civilization   \n",
       "99  the fertile crescent has an impressive record ...   \n",
       "\n",
       "                                              context  exact_match  \n",
       "0                                           not_given          0.0  \n",
       "1   ohn of Patmos. As described, only those whose ...          0.0  \n",
       "2   . The Psalmist likewise speaks of the Book of ...          0.0  \n",
       "3   those who are found written in the book and wh...          0.0  \n",
       "4   ng a story in the newspaper of a 12-year-old g...          0.0  \n",
       "..                                                ...          ...  \n",
       "95                                          not_given          0.0  \n",
       "96                                          not_given          0.0  \n",
       "97  atelevel societies. This has earned the region...          0.0  \n",
       "98  e Fertile Crescent in red The Fertile Crescent...          0.0  \n",
       "99  Cuneiform law History of the Middle East The F...          0.0  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(answer):\n",
    "    try:\n",
    "        answer = ast.literal_eval(answer)\n",
    "    except:\n",
    "        pass\n",
    "    if (isinstance(answer, list) == True):\n",
    "        for i in range(len(answer)):\n",
    "            answer[i] = re.sub(r'[^\\w\\s]', '',answer[i])\n",
    "            answer[i] = answer[i].lower()\n",
    "        return answer\n",
    "    else:\n",
    "        answer = re.sub(r'[^\\w\\s]', '', answer)\n",
    "        answer = answer.lower()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(true_ans, user_ans):\n",
    "    if (isinstance(true_ans, list) == True):\n",
    "        marks = []\n",
    "        for i in range(len(true_ans)):\n",
    "            true_embed = model.encode(true_ans[i], convert_to_tensor=True)\n",
    "            user_embed = model.encode(user_ans, convert_to_tensor=True) \n",
    "            cosine_scores = util.pytorch_cos_sim(true_embed, user_embed)\n",
    "            score = round(cosine_scores.item()*10)\n",
    "            marks.append(score)\n",
    "        if (max(marks) > 9):\n",
    "            return 10\n",
    "        else:\n",
    "            return max(marks)\n",
    "    else:\n",
    "        true_embed = model.encode(true_ans, convert_to_tensor=True)\n",
    "        user_embed = model.encode(user_ans, convert_to_tensor=True)\n",
    "        cosine_scores = util.pytorch_cos_sim(true_embed, user_embed)\n",
    "        return round(cosine_scores.item()*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer: ['all people considered righteous before god', 'every person who is destined for heaven or the world to come']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['all people considered righteous before god', 'every person who is destined for heaven or the world to come']\n",
      "User answer: those whose names are written in the book of life from the foundation of the world\n",
      "Marks: 6\n",
      "Correct answer: ['all people considered righteous before god', 'every person who is destined for heaven or the world to come']\n",
      "User answer: only the names of the righteous\n",
      "Marks: 5\n",
      "Correct answer: ['all people considered righteous before god', 'every person who is destined for heaven or the world to come']\n",
      "User answer: those who are found written in the book and who shall escape the troubles preparatory to the coming of the messianic kingdom are they who together with the risen martyrs\n",
      "Marks: 5\n",
      "Correct answer: ['ethel edy proctor']\n",
      "User answer: her cousin\n",
      "Marks: 3\n",
      "Correct answer: ['ethel edy proctor']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['ethel edy proctor']\n",
      "User answer: her cousin\n",
      "Marks: 3\n",
      "Correct answer: ['ethel edy proctor']\n",
      "User answer: dave hollister\n",
      "Marks: 2\n",
      "Correct answer: ['1971']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['1971']\n",
      "User answer: 1971\n",
      "Marks: 10\n",
      "Correct answer: ['1971']\n",
      "User answer: 1971\n",
      "Marks: 10\n",
      "Correct answer: ['1971']\n",
      "User answer: 2016 learn how and when to remove this template message while this movie is based on a true story it has strayed from the actual events that had occurred on many occasions to add new elements of teamwork commitment and friendship to the film boone may not be the coach that washington portrays in the movie in interviews many former titans football players said they believed his coaching strategies had no correlation to their success and were indeed too harsh causing some players to quit the titans had a solid football team for many years and most of their games were large victories by the end of the 1971\n",
      "Marks: 2\n",
      "Correct answer: ['gettysburg college']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['gettysburg college']\n",
      "User answer: gettysburg college\n",
      "Marks: 10\n",
      "Correct answer: ['gettysburg college']\n",
      "User answer: chicago suntimes\n",
      "Marks: 1\n",
      "Correct answer: ['gettysburg college']\n",
      "User answer: deseret news\n",
      "Marks: 1\n",
      "Correct answer: ['mirror image']\n",
      "User answer: not_given\n",
      "Marks: 2\n",
      "Correct answer: ['mirror image']\n",
      "User answer: mirror image\n",
      "Marks: 10\n",
      "Correct answer: ['mirror image']\n",
      "User answer: mirror image\n",
      "Marks: 10\n",
      "Correct answer: ['mirror image']\n",
      "User answer: the series finale episode which featured the original theme music\n",
      "Marks: 1\n",
      "Correct answer: ['off the rez']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['off the rez']\n",
      "User answer: the locker room\n",
      "Marks: 3\n",
      "Correct answer: ['off the rez']\n",
      "User answer: running away\n",
      "Marks: 3\n",
      "Correct answer: ['off the rez']\n",
      "User answer: reardan\n",
      "Marks: 3\n",
      "Correct answer: ['minor key symphonies', 'g minor']\n",
      "User answer: middle work\n",
      "Marks: 2\n",
      "Correct answer: ['minor key symphonies', 'g minor']\n",
      "User answer: two versions\n",
      "Marks: 1\n",
      "Correct answer: ['minor key symphonies', 'g minor']\n",
      "User answer: g minor\n",
      "Marks: 10\n",
      "Correct answer: ['minor key symphonies', 'g minor']\n",
      "User answer: not_given\n",
      "Marks: 3\n",
      "Correct answer: ['a construction budget of us23 billion']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['a construction budget of us23 billion']\n",
      "User answer: disneyland resort paris\n",
      "Marks: 2\n",
      "Correct answer: ['a construction budget of us23 billion']\n",
      "User answer: disneyland paris\n",
      "Marks: 2\n",
      "Correct answer: ['a construction budget of us23 billion']\n",
      "User answer: four\n",
      "Marks: 1\n",
      "Correct answer: ['1992']\n",
      "User answer: october 27 2009\n",
      "Marks: 4\n",
      "Correct answer: ['1992']\n",
      "User answer: 1992\n",
      "Marks: 10\n",
      "Correct answer: ['1992']\n",
      "User answer: july 18 1992\n",
      "Marks: 5\n",
      "Correct answer: ['1992']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['san cristóbal pinar del río province now in artemisa province in western cuba']\n",
      "User answer: palm trees\n",
      "Marks: 3\n",
      "Correct answer: ['san cristóbal pinar del río province now in artemisa province in western cuba']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['san cristóbal pinar del río province now in artemisa province in western cuba']\n",
      "User answer: caribbean region\n",
      "Marks: 5\n",
      "Correct answer: ['san cristóbal pinar del río province now in artemisa province in western cuba']\n",
      "User answer: cuba\n",
      "Marks: 6\n",
      "Correct answer: ['vienna except from 1583 to 1611 when it was moved to prague']\n",
      "User answer: vienna\n",
      "Marks: 6\n",
      "Correct answer: ['vienna except from 1583 to 1611 when it was moved to prague']\n",
      "User answer: vienna\n",
      "Marks: 6\n",
      "Correct answer: ['vienna except from 1583 to 1611 when it was moved to prague']\n",
      "User answer: not_given\n",
      "Marks: 0\n",
      "Correct answer: ['vienna except from 1583 to 1611 when it was moved to prague']\n",
      "User answer: vienna austrias capital became a state january 1 1922 after being residence and capital of the austrian empire reichshaupt und residenzstadt wien\n",
      "Marks: 6\n",
      "Correct answer: ['ray charles']\n",
      "User answer: ray charles\n",
      "Marks: 10\n",
      "Correct answer: ['ray charles']\n",
      "User answer: ray charles\n",
      "Marks: 10\n",
      "Correct answer: ['ray charles']\n",
      "User answer: eddie cochran\n",
      "Marks: 4\n",
      "Correct answer: ['ray charles']\n",
      "User answer: guy sebastian\n",
      "Marks: 4\n",
      "Correct answer: ['since at least 1997 and in the middle of the eighth inning at every game since 2002', '1997', 'at least 1997']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['since at least 1997 and in the middle of the eighth inning at every game since 2002', '1997', 'at least 1997']\n",
      "User answer: 1997\n",
      "Marks: 10\n",
      "Correct answer: ['since at least 1997 and in the middle of the eighth inning at every game since 2002', '1997', 'at least 1997']\n",
      "User answer: april 25 2013\n",
      "Marks: 6\n",
      "Correct answer: ['since at least 1997 and in the middle of the eighth inning at every game since 2002', '1997', 'at least 1997']\n",
      "User answer: 2018 australian open womens final\n",
      "Marks: 2\n",
      "Correct answer: ['bernard hill']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['bernard hill']\n",
      "User answer: phil parsons\n",
      "Marks: 4\n",
      "Correct answer: ['bernard hill']\n",
      "User answer: hill\n",
      "Marks: 7\n",
      "Correct answer: ['bernard hill']\n",
      "User answer: luther plunkitt\n",
      "Marks: 4\n",
      "Correct answer: ['lasted from 28 july 1914 to 11 november 1918', '28 july 1914 to 11 november 1918']\n",
      "User answer: 28 july 1914 to 11 november 1918\n",
      "Marks: 10\n",
      "Correct answer: ['lasted from 28 july 1914 to 11 november 1918', '28 july 1914 to 11 november 1918']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['lasted from 28 july 1914 to 11 november 1918', '28 july 1914 to 11 november 1918']\n",
      "User answer: 1 july 2013\n",
      "Marks: 4\n",
      "Correct answer: ['lasted from 28 july 1914 to 11 november 1918', '28 july 1914 to 11 november 1918']\n",
      "User answer: germany paid off the final amount of reparations imposed on it by the allies\n",
      "Marks: 4\n",
      "Correct answer: ['16']\n",
      "User answer: not_given\n",
      "Marks: 2\n",
      "Correct answer: ['16']\n",
      "User answer: 16\n",
      "Marks: 10\n",
      "Correct answer: ['16']\n",
      "User answer: 16\n",
      "Marks: 10\n",
      "Correct answer: ['16']\n",
      "User answer: two\n",
      "Marks: 3\n",
      "Correct answer: ['jesus christ']\n",
      "User answer: not_given\n",
      "Marks: 2\n",
      "Correct answer: ['jesus christ']\n",
      "User answer: reverend robert wilkins\n",
      "Marks: 1\n",
      "Correct answer: ['jesus christ']\n",
      "User answer: pompeo batoni\n",
      "Marks: 1\n",
      "Correct answer: ['jesus christ']\n",
      "User answer: henri nouwen\n",
      "Marks: 1\n",
      "Correct answer: ['alastair cook']\n",
      "User answer: sunil gavaskar\n",
      "Marks: 5\n",
      "Correct answer: ['alastair cook']\n",
      "User answer: not_given\n",
      "Marks: 2\n",
      "Correct answer: ['alastair cook']\n",
      "User answer: 1987gavaskar\n",
      "Marks: 2\n",
      "Correct answer: ['alastair cook']\n",
      "User answer: sunil gavaskar\n",
      "Marks: 5\n",
      "Correct answer: ['lament on various worldwide problems']\n",
      "User answer: where is the love\n",
      "Marks: 1\n",
      "Correct answer: ['lament on various worldwide problems']\n",
      "User answer: where is the love\n",
      "Marks: 1\n",
      "Correct answer: ['lament on various worldwide problems']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['lament on various worldwide problems']\n",
      "User answer: where is the love\n",
      "Marks: 1\n",
      "Correct answer: ['ceramic art', 'ceramic']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['ceramic art', 'ceramic']\n",
      "User answer: ceramic art\n",
      "Marks: 10\n",
      "Correct answer: ['ceramic art', 'ceramic']\n",
      "User answer: native american pottery\n",
      "Marks: 7\n",
      "Correct answer: ['ceramic art', 'ceramic']\n",
      "User answer: earthenware\n",
      "Marks: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer: ['the 90s', 'around 2011']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['the 90s', 'around 2011']\n",
      "User answer: 2011\n",
      "Marks: 7\n",
      "Correct answer: ['the 90s', 'around 2011']\n",
      "User answer: 20150119 3j5210 152 finn puts a spell on the compound trapping wolves and vampires klaus takes cami to the safe house then confronts kol about rebekahs whereabouts at the end of the episode it is revealed that the ghost who talked to rebekah is freya 33 11 brotherhood of the damned sylvain white kyle arrington  diane ademujohn january 26 2015\n",
      "Marks: 1\n",
      "Correct answer: ['the 90s', 'around 2011']\n",
      "User answer: 32 10 gonna set your flag on fire rob hardy ashley lyle  bart nickerson january 19 2015\n",
      "Marks: 3\n",
      "Correct answer: ['an apprentice named jeanpaul valley aka azrael', 'jeanpaul valley aka azrael']\n",
      "User answer: not_given\n",
      "Marks: 2\n",
      "Correct answer: ['an apprentice named jeanpaul valley aka azrael', 'jeanpaul valley aka azrael']\n",
      "User answer: zoom\n",
      "Marks: 2\n",
      "Correct answer: ['an apprentice named jeanpaul valley aka azrael', 'jeanpaul valley aka azrael']\n",
      "User answer: bane\n",
      "Marks: 2\n",
      "Correct answer: ['an apprentice named jeanpaul valley aka azrael', 'jeanpaul valley aka azrael']\n",
      "User answer: bane\n",
      "Marks: 2\n",
      "Correct answer: ['danishnorwegian patronymic surname meaning son of anders']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['danishnorwegian patronymic surname meaning son of anders']\n",
      "User answer: denmark\n",
      "Marks: 4\n",
      "Correct answer: ['danishnorwegian patronymic surname meaning son of anders']\n",
      "User answer: apartment\n",
      "Marks: 1\n",
      "Correct answer: ['danishnorwegian patronymic surname meaning son of anders']\n",
      "User answer: greek\n",
      "Marks: 1\n",
      "Correct answer: ['tigris and euphrates rivers']\n",
      "User answer: tigris and euphrates\n",
      "Marks: 9\n",
      "Correct answer: ['tigris and euphrates rivers']\n",
      "User answer: tigris and euphrates rivers\n",
      "Marks: 10\n",
      "Correct answer: ['tigris and euphrates rivers']\n",
      "User answer: jordan river\n",
      "Marks: 5\n",
      "Correct answer: ['tigris and euphrates rivers']\n",
      "User answer: not_given\n",
      "Marks: 0\n",
      "Correct answer: ['popularized by university of chicago archaeologist james henry breasted beginning with his high school textbooks outlines of european history in 1914 and ancient times a history of the early world in 1916']\n",
      "User answer: not_given\n",
      "Marks: 1\n",
      "Correct answer: ['popularized by university of chicago archaeologist james henry breasted beginning with his high school textbooks outlines of european history in 1914 and ancient times a history of the early world in 1916']\n",
      "User answer: the cradle of civilization\n",
      "Marks: 4\n",
      "Correct answer: ['popularized by university of chicago archaeologist james henry breasted beginning with his high school textbooks outlines of european history in 1914 and ancient times a history of the early world in 1916']\n",
      "User answer: the cradle of civilization\n",
      "Marks: 4\n",
      "Correct answer: ['popularized by university of chicago archaeologist james henry breasted beginning with his high school textbooks outlines of european history in 1914 and ancient times a history of the early world in 1916']\n",
      "User answer: the fertile crescent has an impressive record of past human activity\n",
      "Marks: 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    marks = evaluation(df['gold_answers'][i], df['answer'][i])\n",
    "    print('Correct answer:',df['gold_answers'][i])\n",
    "    print('User answer:',df['answer'][i])\n",
    "    print('Marks:',marks)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what sits on top of the main building at notre dame ?\n",
      "Correct answer: a golden statue of the virgin mary .\n",
      "Student answer: statue of Virgin Mary\n",
      "Marks: 9\n"
     ]
    }
   ],
   "source": [
    "print('Question:',preprocessor(sq['train'][4]['question']), '?')\n",
    "print('Correct answer:',preprocessor(sq['train'][4]['answers']['text'][0]),'.')\n",
    "print('Student answer:','statue of Virgin Mary')\n",
    "print('Marks:',evaluation(sq['train'][4]['answers']['text'][0], 'statue of Virgin Mary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
